{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data_iob_skrypt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 newsgroups dataset\n",
    "klasyfikacja postów do 20 kategorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "#train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "#test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_iob_skrypt.load_train_data()\n",
    "test = data_iob_skrypt.load_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kategorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train\n",
    "#train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:  prus\n",
      "\n",
      "Rozumiała ona, co znaczy: duży kłopot, i rada była przynajmniej dowiedzieć się o przyszłych losach nowej przyjaciółki. Zbliżyła się ostrożnie do płotu i położywszy palec na ustach, to nasłuchiwała, to zaglądała do ogrodu. Ale na wejście tam zabrakło jej odwagi.\n",
      "\n",
      "Wtedy we drzwiach chaty ukazał się człowiek olbrzymiego wzrostu, bosy, z rozpiętą koszulą na piersiach. Włożył obie ręce za pazuchę i patrzył to w stronę powozu, który już znikł, to na ogród, w którym ukryła się Anielka, to na dach i kominy dworu.\n",
      "\n",
      "— Wyrodziła się! — mruknął. Postał jeszcze chwilę i wrócił do izby.\n",
      "\n",
      "Z bijącym sercem zbliżała się Anielka do domu. Trapiły ją dwa zmartwienia. Obraziła rzadko widywanego ojca i przyprawiła o silne wzruszenie swoją nauczycielkę.\n",
      "\n",
      "„Co to będzie, jak ojciec z nią «rozmawiać» zacznie? Panna Walentyna niezawodnie połączy się z nim. Matka zasłabnie jeszcze bardziej…”\n",
      "\n",
      "Nurtował ją męczący niepokój, pod wpływem którego ogród wydał się jej brzydki, a dom straszny.\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "print('Category: ', train.target_names[train.target[n]])\n",
    "print()\n",
    "print(train.data[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zamiana opisów na wektory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(binary=True, min_df=0.01, max_df=0.5)\n",
    "term_matrix = count_vectorizer.fit_transform(train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W słowniku mamy 1371 słów, reprezuntujemy tekst jako wektor 0 i 1, 1 jeśli dane słowo występuje w tekście"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 6548)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W każdym tekście występuje niewiele słów, dla oszczędności pamięci stosowana jest reprezentacja \"sparse\", czyli zamiast całej macierzy przechowujemy indeksy jedynek (w ogólności niezerowych elementów)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x6548 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 56 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 34)\t1\n",
      "  (0, 1396)\t1\n",
      "  (0, 3156)\t1\n",
      "  (0, 32)\t1\n",
      "  (0, 5073)\t1\n",
      "  (0, 3372)\t1\n",
      "  (0, 5644)\t1\n",
      "  (0, 4710)\t1\n",
      "  (0, 1943)\t1\n",
      "  (0, 3958)\t1\n",
      "  (0, 5098)\t1\n",
      "  (0, 1157)\t1\n",
      "  (0, 5845)\t1\n",
      "  (0, 3789)\t1\n",
      "  (0, 2065)\t1\n",
      "  (0, 5615)\t1\n",
      "  (0, 5444)\t1\n",
      "  (0, 4700)\t1\n",
      "  (0, 3224)\t1\n",
      "  (0, 1825)\t1\n",
      "  (0, 2048)\t1\n",
      "  (0, 3181)\t1\n",
      "  (0, 876)\t1\n",
      "  (0, 4812)\t1\n",
      "  (0, 2851)\t1\n",
      "  :\t:\n",
      "  (0, 5432)\t1\n",
      "  (0, 5378)\t1\n",
      "  (0, 2235)\t1\n",
      "  (0, 831)\t1\n",
      "  (0, 2418)\t1\n",
      "  (0, 2102)\t1\n",
      "  (0, 694)\t1\n",
      "  (0, 282)\t1\n",
      "  (0, 3984)\t1\n",
      "  (0, 2254)\t1\n",
      "  (0, 6457)\t1\n",
      "  (0, 3324)\t1\n",
      "  (0, 6404)\t1\n",
      "  (0, 3677)\t1\n",
      "  (0, 415)\t1\n",
      "  (0, 1762)\t1\n",
      "  (0, 2603)\t1\n",
      "  (0, 2001)\t1\n",
      "  (0, 1636)\t1\n",
      "  (0, 5361)\t1\n",
      "  (0, 2482)\t1\n",
      "  (0, 6445)\t1\n",
      "  (0, 3843)\t1\n",
      "  (0, 87)\t1\n",
      "  (0, 2415)\t1\n"
     ]
    }
   ],
   "source": [
    "print(term_matrix[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '1655', '25', 'abu', 'aby', 'ach', 'afryce', 'afryka', 'afryki', 'akacji', 'akacyj', 'aksamitu', 'albo', 'albowiem', 'alei', 'aleksandra', 'aleksandrze', 'aleksandrą', 'aleksandrę', 'allach', 'andrzej', 'andrzeja', 'andrzejowi', 'angielscy', 'angielski', 'angielskich', 'angielsku', 'angielską', 'anglia', 'anglicy', 'anglii', 'anglików', 'ani', 'anielce', 'anielka', 'anielki', 'anielkę', 'aniżeli', 'anny', 'antek', 'antylop', 'anusi', 'anusia', 'arab', 'arabom', 'arabowie', 'arabski', 'arabskie', 'arabsku', 'arabską', 'arabów', 'aresztować', 'armat', 'armatami', 'armatek', 'armaty', 'armia', 'artylerii', 'arystokratyczne', 'assuan', 'assuanem', 'assuanu', 'astmy', 'atak', 'ataku', 'atamana', 'atamani', 'atamanów', 'aż', 'ażeby', 'ba', 'baczenie', 'bagien', 'bagniskach', 'bahr', 'bajdakami', 'bajdaki', 'bali', 'balu', 'barabasz', 'barabasza', 'baranie', 'bardziej', 'bardzo', 'barw', 'barwy', 'barwę', 'bawiąc', 'bawić', 'bawił', 'bawoły', 'bać', 'bał', 'beczek', 'beczki', 'beduinami', 'beduini', 'beduinów', 'bej', 'beja']\n"
     ]
    }
   ],
   "source": [
    "# pierwsze 100 słów ze słownika\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "print(feature_names[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że występują tu również liczby, a słowa powtarzają się wielokrotnie w różnych odmianach\n",
    "(dla języka polskiego to jest jeszcze bardziej problematyczne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(term_matrix, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(term_matrix, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix_test = count_vectorizer.transform(test.data)\n",
    "clf.score(term_matrix_test, test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sien :  nawet tego lub której były nad była nim przed by\n",
      "prus :  może lecz albo tam ten on nawet ją jest jej\n"
     ]
    }
   ],
   "source": [
    "# wyświetlamy 10 najważniejszych (najczęściej występujących) słów dla każdej kategorii\n",
    "feature_names_arr = np.asarray(feature_names)\n",
    "for i, category in enumerate(train.target_names):\n",
    "    top10 = np.argsort(clf.feature_log_prob_[i])[-10:]\n",
    "    print(category, ': ', \" \".join(feature_names_arr[top10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dostajemy prawie wyłącznie częste słowa, bez dużego znaczenia dla klasyfikacji tzw. stopwords - dobrze jest je usunąć"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wysoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aby',\n",
       " 'ach',\n",
       " 'acz',\n",
       " 'aczkolwiek',\n",
       " 'aj',\n",
       " 'albo',\n",
       " 'ale',\n",
       " 'alez',\n",
       " 'ależ',\n",
       " 'ani',\n",
       " 'az',\n",
       " 'aż',\n",
       " 'bardziej',\n",
       " 'bardzo',\n",
       " 'beda',\n",
       " 'bedzie',\n",
       " 'bez',\n",
       " 'deda',\n",
       " 'będą',\n",
       " 'bede',\n",
       " 'będę',\n",
       " 'będzie',\n",
       " 'bo',\n",
       " 'bowiem',\n",
       " 'by',\n",
       " 'byc',\n",
       " 'być',\n",
       " 'byl',\n",
       " 'byla',\n",
       " 'byli',\n",
       " 'bylo',\n",
       " 'byly',\n",
       " 'był',\n",
       " 'była',\n",
       " 'było',\n",
       " 'były',\n",
       " 'bynajmniej',\n",
       " 'cala',\n",
       " 'cali',\n",
       " 'caly',\n",
       " 'cała',\n",
       " 'cały',\n",
       " 'ci',\n",
       " 'cie',\n",
       " 'ciebie',\n",
       " 'cię',\n",
       " 'co',\n",
       " 'cokolwiek',\n",
       " 'cos',\n",
       " 'coś',\n",
       " 'czasami',\n",
       " 'czasem',\n",
       " 'czemu',\n",
       " 'czy',\n",
       " 'czyli',\n",
       " 'daleko',\n",
       " 'dla',\n",
       " 'dlaczego',\n",
       " 'dlatego',\n",
       " 'do',\n",
       " 'dobrze',\n",
       " 'dokad',\n",
       " 'dokąd',\n",
       " 'dosc',\n",
       " 'dość',\n",
       " 'duzo',\n",
       " 'dużo',\n",
       " 'dwa',\n",
       " 'dwaj',\n",
       " 'dwie',\n",
       " 'dwoje',\n",
       " 'dzis',\n",
       " 'dzisiaj',\n",
       " 'dziś',\n",
       " 'gdy',\n",
       " 'gdyby',\n",
       " 'gdyz',\n",
       " 'gdyż',\n",
       " 'gdzie',\n",
       " 'gdziekolwiek',\n",
       " 'gdzies',\n",
       " 'gdzieś',\n",
       " 'go',\n",
       " 'i',\n",
       " 'ich',\n",
       " 'ile',\n",
       " 'im',\n",
       " 'inna',\n",
       " 'inne',\n",
       " 'inny',\n",
       " 'innych',\n",
       " 'iz',\n",
       " 'iż',\n",
       " 'ja',\n",
       " 'jak',\n",
       " 'jakas',\n",
       " 'jakaś',\n",
       " 'jakby',\n",
       " 'jaki',\n",
       " 'jakichs',\n",
       " 'jakichś',\n",
       " 'jakie',\n",
       " 'jakis',\n",
       " 'jakiś',\n",
       " 'jakiz',\n",
       " 'jakiż',\n",
       " 'jakkolwiek',\n",
       " 'jako',\n",
       " 'jakos',\n",
       " 'jakoś',\n",
       " 'ją',\n",
       " 'je',\n",
       " 'jeden',\n",
       " 'jedna',\n",
       " 'jednak',\n",
       " 'jednakze',\n",
       " 'jednakże',\n",
       " 'jedno',\n",
       " 'jego',\n",
       " 'jej',\n",
       " 'jemu',\n",
       " 'jesli',\n",
       " 'jest',\n",
       " 'jestem',\n",
       " 'jeszcze',\n",
       " 'jeśli',\n",
       " 'jezeli',\n",
       " 'jeżeli',\n",
       " 'juz',\n",
       " 'już',\n",
       " 'kazdy',\n",
       " 'każdy',\n",
       " 'kiedy',\n",
       " 'kilka',\n",
       " 'kims',\n",
       " 'kimś',\n",
       " 'kto',\n",
       " 'ktokolwiek',\n",
       " 'ktora',\n",
       " 'ktore',\n",
       " 'ktorego',\n",
       " 'ktorej',\n",
       " 'ktory',\n",
       " 'ktorych',\n",
       " 'ktorym',\n",
       " 'ktorzy',\n",
       " 'ktos',\n",
       " 'ktoś',\n",
       " 'która',\n",
       " 'które',\n",
       " 'którego',\n",
       " 'której',\n",
       " 'który',\n",
       " 'których',\n",
       " 'którym',\n",
       " 'którzy',\n",
       " 'ku',\n",
       " 'lat',\n",
       " 'lecz',\n",
       " 'lub',\n",
       " 'ma',\n",
       " 'mają',\n",
       " 'mało',\n",
       " 'mam',\n",
       " 'mi',\n",
       " 'miedzy',\n",
       " 'między',\n",
       " 'mimo',\n",
       " 'mna',\n",
       " 'mną',\n",
       " 'mnie',\n",
       " 'moga',\n",
       " 'mogą',\n",
       " 'moi',\n",
       " 'moim',\n",
       " 'moj',\n",
       " 'moja',\n",
       " 'moje',\n",
       " 'moze',\n",
       " 'mozliwe',\n",
       " 'mozna',\n",
       " 'może',\n",
       " 'możliwe',\n",
       " 'można',\n",
       " 'mój',\n",
       " 'mu',\n",
       " 'musi',\n",
       " 'my',\n",
       " 'na',\n",
       " 'nad',\n",
       " 'nam',\n",
       " 'nami',\n",
       " 'nas',\n",
       " 'nasi',\n",
       " 'nasz',\n",
       " 'nasza',\n",
       " 'nasze',\n",
       " 'naszego',\n",
       " 'naszych',\n",
       " 'natomiast',\n",
       " 'natychmiast',\n",
       " 'nawet',\n",
       " 'nia',\n",
       " 'nią',\n",
       " 'nic',\n",
       " 'nich',\n",
       " 'nie',\n",
       " 'niech',\n",
       " 'niego',\n",
       " 'niej',\n",
       " 'niemu',\n",
       " 'nigdy',\n",
       " 'nim',\n",
       " 'nimi',\n",
       " 'niz',\n",
       " 'niż',\n",
       " 'no',\n",
       " 'o',\n",
       " 'obok',\n",
       " 'od',\n",
       " 'około',\n",
       " 'on',\n",
       " 'ona',\n",
       " 'one',\n",
       " 'oni',\n",
       " 'ono',\n",
       " 'oraz',\n",
       " 'oto',\n",
       " 'owszem',\n",
       " 'pan',\n",
       " 'pana',\n",
       " 'pani',\n",
       " 'po',\n",
       " 'pod',\n",
       " 'podczas',\n",
       " 'pomimo',\n",
       " 'ponad',\n",
       " 'poniewaz',\n",
       " 'ponieważ',\n",
       " 'powinien',\n",
       " 'powinna',\n",
       " 'powinni',\n",
       " 'powinno',\n",
       " 'poza',\n",
       " 'prawie',\n",
       " 'przeciez',\n",
       " 'przecież',\n",
       " 'przed',\n",
       " 'przede',\n",
       " 'przedtem',\n",
       " 'przez',\n",
       " 'przy',\n",
       " 'roku',\n",
       " 'rowniez',\n",
       " 'również',\n",
       " 'sam',\n",
       " 'sama',\n",
       " 'są',\n",
       " 'sie',\n",
       " 'się',\n",
       " 'skad',\n",
       " 'skąd',\n",
       " 'soba',\n",
       " 'sobą',\n",
       " 'sobie',\n",
       " 'sposob',\n",
       " 'sposób',\n",
       " 'swoje',\n",
       " 'ta',\n",
       " 'tak',\n",
       " 'taka',\n",
       " 'taki',\n",
       " 'takie',\n",
       " 'takze',\n",
       " 'także',\n",
       " 'tam',\n",
       " 'te',\n",
       " 'tego',\n",
       " 'tej',\n",
       " 'ten',\n",
       " 'teraz',\n",
       " 'też',\n",
       " 'to',\n",
       " 'toba',\n",
       " 'tobą',\n",
       " 'tobie',\n",
       " 'totez',\n",
       " 'toteż',\n",
       " 'totobą',\n",
       " 'trzeba',\n",
       " 'tu',\n",
       " 'tutaj',\n",
       " 'twoi',\n",
       " 'twoim',\n",
       " 'twoj',\n",
       " 'twoja',\n",
       " 'twoje',\n",
       " 'twój',\n",
       " 'twym',\n",
       " 'ty',\n",
       " 'tych',\n",
       " 'tylko',\n",
       " 'tym',\n",
       " 'u',\n",
       " 'w',\n",
       " 'wam',\n",
       " 'wami',\n",
       " 'was',\n",
       " 'wasz',\n",
       " 'wasza',\n",
       " 'wasze',\n",
       " 'we',\n",
       " 'według',\n",
       " 'wiele',\n",
       " 'wielu',\n",
       " 'więc',\n",
       " 'więcej',\n",
       " 'wlasnie',\n",
       " 'właśnie',\n",
       " 'wszyscy',\n",
       " 'wszystkich',\n",
       " 'wszystkie',\n",
       " 'wszystkim',\n",
       " 'wszystko',\n",
       " 'wtedy',\n",
       " 'wy',\n",
       " 'z',\n",
       " 'za',\n",
       " 'zaden',\n",
       " 'zadna',\n",
       " 'zadne',\n",
       " 'zadnych',\n",
       " 'zapewne',\n",
       " 'zawsze',\n",
       " 'ze',\n",
       " 'zeby',\n",
       " 'zeznowu',\n",
       " 'zł',\n",
       " 'znow',\n",
       " 'znowu',\n",
       " 'znów',\n",
       " 'zostal',\n",
       " 'został',\n",
       " 'żaden',\n",
       " 'żadna',\n",
       " 'żadne',\n",
       " 'żadnych',\n",
       " 'że',\n",
       " 'żeby']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('polish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 6343)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(binary=True, min_df=0.01, max_df=0.5, stop_words=stopwords.words('polish'))\n",
    "term_matrix = count_vectorizer.fit_transform(train.data)\n",
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '1655', '25', 'abu', 'afryce', 'afryka', 'afryki', 'akacji', 'akacyj', 'aksamitu', 'albowiem', 'alei', 'aleksandra', 'aleksandrze', 'aleksandrą', 'aleksandrę', 'allach', 'andrzej', 'andrzeja', 'andrzejowi', 'angielscy', 'angielski', 'angielskich', 'angielsku', 'angielską', 'anglia', 'anglicy', 'anglii', 'anglików', 'anielce', 'anielka', 'anielki', 'anielkę', 'aniżeli', 'anny', 'antek', 'antylop', 'anusi', 'anusia', 'arab', 'arabom', 'arabowie', 'arabski', 'arabskie', 'arabsku', 'arabską', 'arabów', 'aresztować', 'armat', 'armatami', 'armatek', 'armaty', 'armia', 'artylerii', 'arystokratyczne', 'assuan', 'assuanem', 'assuanu', 'astmy', 'atak', 'ataku', 'atamana', 'atamani', 'atamanów', 'ażeby', 'ba', 'baczenie', 'bagien', 'bagniskach', 'bahr', 'bajdakami', 'bajdaki', 'bali', 'balu', 'barabasz', 'barabasza', 'baranie', 'barw', 'barwy', 'barwę', 'bawiąc', 'bawić', 'bawił', 'bawoły', 'bać', 'bał', 'beczek', 'beczki', 'beduinami', 'beduini', 'beduinów', 'bej', 'beja', 'bezkarność', 'bezmyślnie', 'bezpieczny', 'bezład', 'biada', 'białe', 'białego']\n"
     ]
    }
   ],
   "source": [
    "feature_names = count_vectorizer.get_feature_names()\n",
    "print(feature_names[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = BernoulliNB()\n",
    "clf.fit(term_matrix, train.target)\n",
    "clf.score(term_matrix, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix_test = count_vectorizer.transform(test.data)\n",
    "clf.score(term_matrix_test, test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sien :  naprzód ludzie zaraz oczy coraz tymczasem mógł miał zaś ludzi\n",
      "prus :  ręce zaś potem dzień ażeby anielka panna ludzie domu miał\n"
     ]
    }
   ],
   "source": [
    "feature_names_arr = np.asarray(feature_names)\n",
    "import numpy as np\n",
    "for i, category in enumerate(train.target_names):\n",
    "    top10 = np.argsort(clf.feature_log_prob_[i])[-10:]\n",
    "    print(category, ': ', \" \".join(feature_names_arr[top10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 6343)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count_vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.5, stop_words=stopwords.words('polish'))\n",
    "term_matrix = count_vectorizer.fit_transform(train.data)\n",
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2298)\t0.1526413737326487\n",
      "  (0, 80)\t0.17033101103978304\n",
      "  (0, 3691)\t0.1603687239408727\n",
      "  (0, 6245)\t0.1603687239408727\n",
      "  (0, 2361)\t0.17033101103978304\n",
      "  (0, 5177)\t0.11698616536616038\n",
      "  (0, 1563)\t0.17033101103978304\n",
      "  (0, 1911)\t0.17033101103978304\n",
      "  (0, 2476)\t0.17033101103978304\n",
      "  (0, 395)\t0.14098950751288328\n",
      "  (0, 3525)\t0.1603687239408727\n",
      "  (0, 6204)\t0.1603687239408727\n",
      "  (0, 3183)\t0.14632766889306018\n",
      "  (0, 6257)\t0.14632766889306018\n",
      "  (0, 3831)\t0.1603687239408727\n",
      "  (0, 267)\t0.11236203964742696\n",
      "  (0, 663)\t0.12863803158592582\n",
      "  (0, 2301)\t0.12533748745897608\n",
      "  (0, 797)\t0.1603687239408727\n",
      "  (0, 2133)\t0.1526413737326487\n",
      "  (0, 5194)\t0.17033101103978304\n",
      "  (0, 5247)\t0.10828327169852475\n",
      "  (0, 2055)\t0.1603687239408727\n",
      "  (0, 2516)\t0.17033101103978304\n",
      "  (0, 1984)\t0.14098950751288328\n",
      "  :\t:\n",
      "  (149, 638)\t0.05399594886325751\n",
      "  (149, 338)\t0.05031979418367344\n",
      "  (149, 479)\t0.028829387780864114\n",
      "  (149, 6166)\t0.037987476698848305\n",
      "  (149, 2919)\t0.056325750781343636\n",
      "  (149, 4843)\t0.03575949629467821\n",
      "  (149, 4030)\t0.04068769679564831\n",
      "  (149, 452)\t0.0373930191724788\n",
      "  (149, 5304)\t0.0772218867835731\n",
      "  (149, 1842)\t0.054847475557843764\n",
      "  (149, 3980)\t0.03926639641528242\n",
      "  (149, 497)\t0.03301068856373013\n",
      "  (149, 2044)\t0.06602137712746026\n",
      "  (149, 5802)\t0.03386290645260216\n",
      "  (149, 2545)\t0.027967587190091476\n",
      "  (149, 3775)\t0.10727848888403463\n",
      "  (149, 3259)\t0.05649689072279524\n",
      "  (149, 385)\t0.03682499088487002\n",
      "  (149, 4023)\t0.02913007300406976\n",
      "  (149, 2481)\t0.04411571683624088\n",
      "  (149, 5155)\t0.05917719787845198\n",
      "  (149, 3364)\t0.06285335255803605\n",
      "  (149, 5902)\t0.03628114147370043\n",
      "  (149, 838)\t0.032604986794116345\n",
      "  (149, 4550)\t0.05202612940882132\n"
     ]
    }
   ],
   "source": [
    "print(term_matrix[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer.get_feature_names()\n",
    "#print(feature_names[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(term_matrix, train.target)\n",
    "clf.score(term_matrix, train.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix_test = count_vectorizer.transform(test.data)\n",
    "clf.score(term_matrix_test, test.target)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
